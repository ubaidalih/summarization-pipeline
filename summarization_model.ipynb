{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random, time, os\n",
    "from torch.nn import Embedding\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from transformers import get_linear_schedule_with_warmup, pipeline\n",
    "from keras.utils import pad_sequences\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "from rouge import Rouge\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "data = pd.read_csv(\"indosum_srl_test.csv\")\n",
    "indosum_data = load_dataset(\"maryantocinn/indosum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split punctuation in every word\n",
    "sentence_summary = []\n",
    "for text in indosum_data[\"validation\"]:\n",
    "    new_text = text[\"summary\"].split(\",\")\n",
    "    new_text = \" ,\".join(new_text)\n",
    "    new_text = new_text.split(\".\")\n",
    "    new_text = \" .\".join(new_text)\n",
    "    new_text = new_text.split(\"?\")\n",
    "    new_text = \" ?\".join(new_text)\n",
    "    new_text = new_text.lower()\n",
    "    sentence_summary.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\"\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model.to(device)\n",
    "\n",
    "def get_word_embeddings(tokens):\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def filter_labels_if_not_in_common(embed1, embed2, labels1, labels2):\n",
    "    common_labels = set(labels1).intersection(set(labels2))\n",
    "\n",
    "    embed1 = [embed for embed, label in zip(embed1, labels1) if label in common_labels]\n",
    "    label1 = [label for label in labels1 if label in common_labels]\n",
    "    embed2 = [embed for embed, label in zip(embed2, labels2) if label in common_labels]\n",
    "    label2 = [label for label in labels2 if label in common_labels]\n",
    "\n",
    "    all_elements = set(labels1).union(set(labels2))\n",
    "\n",
    "    return embed1, embed2, label1, label2, len(all_elements)\n",
    "\n",
    "def filter_words_labels(srl_tag):\n",
    "    filtered_labels = [label for label in srl_tag if label != 'O']\n",
    "    filtered_labels = [label[2:] for label in filtered_labels]\n",
    "    return filtered_labels\n",
    "\n",
    "def filter_words_token(row):\n",
    "    tokens = row[\"sentence\"].split()\n",
    "    labels = row[\"srl\"]\n",
    "    filtered_tokens = [token for token, label in zip(tokens, labels) if label != 'O']\n",
    "    return filtered_tokens\n",
    "\n",
    "def sentence_similarity(embeddings1, embeddings2, labels1, labels2, count):\n",
    "    max_similarities = {}\n",
    "\n",
    "    for i, (emb1, label1) in enumerate(zip(embeddings1, labels1)):\n",
    "        for j, (emb2, label2) in enumerate(zip(embeddings2, labels2)):\n",
    "            if label1 == label2:\n",
    "                # Calculate similarity\n",
    "                similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "                if label1 not in max_similarities or similarity > max_similarities[label1]:\n",
    "                    max_similarities[label1] = similarity\n",
    "\n",
    "    # Sum up the maximum similarities for each label\n",
    "    total_max_similarity = sum(max_similarities.values())\n",
    "\n",
    "    return total_max_similarity / count\n",
    "\n",
    "def count_o_labels(srl_tags):\n",
    "    return srl_tags.count('O')\n",
    "\n",
    "def reduce_same_sentence(data):\n",
    "    data['o_label_count'] = data['srl'].apply(count_o_labels)\n",
    "    df_sorted = data.sort_values(by='o_label_count', ascending=True)\n",
    "    df_reduced = df_sorted.drop_duplicates(subset='sentence', keep='first')\n",
    "    return df_reduced.sort_index()\n",
    "\n",
    "def calculate_sentence_scores(df):\n",
    "    sentence_scores = []\n",
    "    similarity_cache = {}  # Cache to store previously computed similarities\n",
    "    for i, row1 in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing sentences\"):\n",
    "        embeddings1, labels1 = row1['embeddings'], row1['srl']\n",
    "        sentence_score = 0\n",
    "        \n",
    "        for j, row2 in df.iterrows():\n",
    "            if i != j:\n",
    "                # Check if this similarity has been computed before\n",
    "                if (i, j) in similarity_cache:\n",
    "                    similarity = similarity_cache[(i, j)]\n",
    "                elif (j, i) in similarity_cache:\n",
    "                    similarity = similarity_cache[(j, i)]\n",
    "                else:\n",
    "                    embeddings2, labels2 = row2['embeddings'], row2['srl']\n",
    "                    embeddings1, embeddings2, labels1, labels2, count = filter_labels_if_not_in_common(embeddings1, embeddings2, labels1, labels2)\n",
    "                    similarity = sentence_similarity(embeddings1, embeddings2, labels1, labels2, count)\n",
    "                    similarity_cache[(i, j)] = similarity\n",
    "                    similarity_cache[(j, i)] = similarity\n",
    "                    \n",
    "                sentence_score += similarity\n",
    "        \n",
    "        sentence_scores.append(sentence_score)\n",
    "    \n",
    "    return sentence_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_article = len(data[\"article_id\"].value_counts())\n",
    "list_sentence_hyp = []\n",
    "for i in tqdm(length_article, desc=\"Processing articles\"):\n",
    "    df = data[data[\"article_id\"] == i]\n",
    "    df[\"srl\"] = df[\"srl\"].apply(ast.literal_eval)\n",
    "    df = reduce_same_sentence(df)\n",
    "    df[\"labels\"] =  df['srl'].apply(filter_words_labels)\n",
    "    df[\"token\"] = df.apply(filter_words_token, axis=1)\n",
    "    df['embeddings'] = df[\"token\"].apply(get_word_embeddings)\n",
    "    # Calculate sentence scores\n",
    "    score = calculate_sentence_scores(df)\n",
    "    df[\"score\"] = score\n",
    "    df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    top_count = math.ceil(len(df) / 4)\n",
    "    top = df.head(top_count)\n",
    "    top.sort_values(\"sentence_id\", inplace=True)\n",
    "    sentences = \"\"\n",
    "    for i, sentence in top.iterrows():\n",
    "        sentences += \" \" + sentence[\"sentence\"] + \" .\"\n",
    "    list_sentence_hyp.append(sentences.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Rouge object\n",
    "rouge = Rouge()\n",
    "f1_scores = []\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "for system, reference in zip(list_sentence_hyp, sentence_summary):\n",
    "    scores = rouge.get_scores(system, reference)[0]  # get_scores returns a list of results\n",
    "    f1_rouge1 = scores['rouge-1']['f']\n",
    "    f1_rouge2 = scores['rouge-2']['f']\n",
    "    f1_rougeL = scores['rouge-l']['f']\n",
    "    \n",
    "    # Collect F1 scores for all three ROUGE metrics\n",
    "    f1_scores.append((f1_rouge1, f1_rouge2, f1_rougeL))\n",
    "\n",
    "# Convert list of tuples to a NumPy array for easy averaging\n",
    "f1_scores_array = np.array(f1_scores)\n",
    "\n",
    "# Calculate average F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "average_f1_scores = np.mean(f1_scores_array, axis=0)\n",
    "print(f\"Average F1 Scores: ROUGE-1: {average_f1_scores[0]:.4f}, ROUGE-2: {average_f1_scores[1]:.4f}, ROUGE-L: {average_f1_scores[2]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
