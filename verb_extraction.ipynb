{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seacrowd in /raid/data/tts2024/env/lib/python3.10/site-packages (0.2.2)\n",
      "Collecting nusacrowd\n",
      "  Downloading nusacrowd-0.1.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: loguru>=0.5.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (0.7.2)\n",
      "Requirement already satisfied: bioc>=1.3.7 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (2.1)\n",
      "Requirement already satisfied: pandas>=1.3.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.20 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (1.26.3)\n",
      "Requirement already satisfied: datasets>=2.2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (3.1.0)\n",
      "Requirement already satisfied: black~=22.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (22.12.0)\n",
      "Requirement already satisfied: flake8>=3.8.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (7.1.1)\n",
      "Requirement already satisfied: isort>=5.0.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (5.13.2)\n",
      "Requirement already satisfied: aiohttp>=3.8.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (3.10.10)\n",
      "Requirement already satisfied: pre-commit>=2.19.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (4.0.1)\n",
      "Requirement already satisfied: jsonlines>=3.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (4.0.0)\n",
      "Requirement already satisfied: torchaudio>=0.11 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (2.5.0+cu118)\n",
      "Requirement already satisfied: soundfile in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (0.12.1)\n",
      "Requirement already satisfied: librosa in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (0.10.2.post1)\n",
      "Requirement already satisfied: nltk in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (3.9.1)\n",
      "Requirement already satisfied: zstandard in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (0.23.0)\n",
      "Requirement already satisfied: ffmpeg in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (1.4)\n",
      "Requirement already satisfied: conllu in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (6.0.0)\n",
      "Requirement already satisfied: openpyxl in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (3.1.5)\n",
      "Requirement already satisfied: translate-toolkit>=3.7.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (3.14.1)\n",
      "Requirement already satisfied: typing-extensions in /raid/data/tts2024/env/lib/python3.10/site-packages (from seacrowd) (4.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from aiohttp>=3.8.1->seacrowd) (4.0.3)\n",
      "Requirement already satisfied: lxml>=4.6.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from bioc>=1.3.7->seacrowd) (5.3.0)\n",
      "Requirement already satisfied: intervaltree in /raid/data/tts2024/env/lib/python3.10/site-packages (from bioc>=1.3.7->seacrowd) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /raid/data/tts2024/env/lib/python3.10/site-packages (from bioc>=1.3.7->seacrowd) (4.66.6)\n",
      "Requirement already satisfied: docopt in /raid/data/tts2024/env/lib/python3.10/site-packages (from bioc>=1.3.7->seacrowd) (0.6.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from black~=22.0->seacrowd) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from black~=22.0->seacrowd) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from black~=22.0->seacrowd) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from black~=22.0->seacrowd) (4.3.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from black~=22.0->seacrowd) (2.2.1)\n",
      "Requirement already satisfied: filelock in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.2.0->seacrowd) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (0.26.2)\n",
      "Requirement already satisfied: packaging in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from datasets>=2.2.0->seacrowd) (6.0.2)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from flake8>=3.8.3->seacrowd) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.13.0,>=2.12.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from flake8>=3.8.3->seacrowd) (2.12.1)\n",
      "Requirement already satisfied: pyflakes<3.3.0,>=3.2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from flake8>=3.8.3->seacrowd) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pandas>=1.3.3->seacrowd) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pandas>=1.3.3->seacrowd) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pandas>=1.3.3->seacrowd) (2024.2)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pre-commit>=2.19.0->seacrowd) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pre-commit>=2.19.0->seacrowd) (2.6.3)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pre-commit>=2.19.0->seacrowd) (1.9.1)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from pre-commit>=2.19.0->seacrowd) (20.28.0)\n",
      "Requirement already satisfied: torch==2.5.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torchaudio>=0.11->seacrowd) (2.5.0+cu118)\n",
      "Requirement already satisfied: networkx in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (11.8.86)\n",
      "Requirement already satisfied: triton==3.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from torch==2.5.0->torchaudio>=0.11->seacrowd) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from sympy==1.13.1->torch==2.5.0->torchaudio>=0.11->seacrowd) (1.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.10 in /raid/data/tts2024/env/lib/python3.10/site-packages (from translate-toolkit>=3.7.3->seacrowd) (0.2.13)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from librosa->seacrowd) (1.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from soundfile->seacrowd) (1.17.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /raid/data/tts2024/env/lib/python3.10/site-packages (from nltk->seacrowd) (2024.9.11)\n",
      "Requirement already satisfied: et-xmlfile in /raid/data/tts2024/env/lib/python3.10/site-packages (from openpyxl->seacrowd) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /raid/data/tts2024/env/lib/python3.10/site-packages (from cffi>=1.0->soundfile->seacrowd) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from numba>=0.51.0->librosa->seacrowd) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /raid/data/tts2024/env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.3->seacrowd) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/data/tts2024/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.2.0->seacrowd) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/data/tts2024/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.2.0->seacrowd) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/data/tts2024/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.2.0->seacrowd) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/data/tts2024/env/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.2.0->seacrowd) (2024.8.30)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->seacrowd) (3.5.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /raid/data/tts2024/env/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit>=2.19.0->seacrowd) (0.3.9)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.8.1->seacrowd) (0.2.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from intervaltree->bioc>=1.3.7->seacrowd) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/data/tts2024/env/lib/python3.10/site-packages (from jinja2->torch==2.5.0->torchaudio>=0.11->seacrowd) (2.1.5)\n",
      "Downloading nusacrowd-0.1.4-py3-none-any.whl (384 kB)\n",
      "Installing collected packages: nusacrowd\n",
      "Successfully installed nusacrowd-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install seacrowd nusacrowd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Pos Tagger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 03:50:28.937416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-07 03:50:28.937478: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-07 03:50:28.937499: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 03:50:28.946418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 03:50:29.850655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Training and Evaluation Pos Tagger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "\n",
    "class EvalCounts(object):\n",
    "    def __init__(self):\n",
    "        self.correct_chunk = 0    # number of correctly identified chunks\n",
    "        self.correct_tags = 0     # number of correct chunk tags\n",
    "        self.found_correct = 0    # number of chunks in corpus\n",
    "        self.found_guessed = 0    # number of identified chunks\n",
    "        self.token_counter = 0    # token counter (ignores sentence breaks)\n",
    "\n",
    "        # counts by type\n",
    "        self.t_correct_chunk = defaultdict(int)\n",
    "        self.t_found_correct = defaultdict(int)\n",
    "        self.t_found_guessed = defaultdict(int)\n",
    "\n",
    "###\n",
    "# Evaluate Function\n",
    "###        \n",
    "def parse_tag(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, '')\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk started between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if tag == '[': chunk_start = True\n",
    "    if tag == ']': chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk ended between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if prev_tag == ']': chunk_end = True\n",
    "    if prev_tag == '[': chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "def evaluate_fn(guessed, correct, last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts):\n",
    "    guessed, guessed_type = parse_tag(guessed)\n",
    "    correct, correct_type = parse_tag(correct)\n",
    "\n",
    "    end_correct = end_of_chunk(last_correct, correct,\n",
    "                               last_correct_type, correct_type)\n",
    "    end_guessed = end_of_chunk(last_guessed, guessed,\n",
    "                               last_guessed_type, guessed_type)\n",
    "    start_correct = start_of_chunk(last_correct, correct,\n",
    "                                   last_correct_type, correct_type)\n",
    "    start_guessed = start_of_chunk(last_guessed, guessed,\n",
    "                                   last_guessed_type, guessed_type)\n",
    "\n",
    "    if in_correct:\n",
    "        if (end_correct and end_guessed and\n",
    "            last_guessed_type == last_correct_type):\n",
    "            in_correct = False\n",
    "            counts.correct_chunk += 1\n",
    "            counts.t_correct_chunk[last_correct_type] += 1\n",
    "        elif (end_correct != end_guessed or guessed_type != correct_type):\n",
    "            in_correct = False\n",
    "\n",
    "    if start_correct and start_guessed and guessed_type == correct_type:\n",
    "        in_correct = True\n",
    "\n",
    "    if start_correct:\n",
    "        counts.found_correct += 1\n",
    "        counts.t_found_correct[correct_type] += 1\n",
    "    if start_guessed:\n",
    "        counts.found_guessed += 1\n",
    "        counts.t_found_guessed[guessed_type] += 1\n",
    "    if correct == guessed and guessed_type == correct_type:\n",
    "        counts.correct_tags += 1\n",
    "    counts.token_counter += 1\n",
    "\n",
    "    last_guessed = guessed\n",
    "    last_correct = correct\n",
    "    last_guessed_type = guessed_type\n",
    "    last_correct_type = correct_type\n",
    "    \n",
    "    return last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts\n",
    "    \n",
    "def evaluate(hyps_list, labels_list):\n",
    "    counts = EvalCounts()\n",
    "    num_features = None       # number of features per line\n",
    "    in_correct = False        # currently processed chunks is correct until now\n",
    "    last_correct = 'O'        # previous chunk tag in corpus\n",
    "    last_correct_type = ''    # type of previously identified chunk tag\n",
    "    last_guessed = 'O'        # previously identified chunk tag\n",
    "    last_guessed_type = ''    # type of previous chunk tag in corpus\n",
    "\n",
    "    for hyps, labels in zip(hyps_list, labels_list):\n",
    "        for hyp, label in zip(hyps, labels):\n",
    "            step_result = evaluate_fn(hyp, label, last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts)\n",
    "            last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts = step_result\n",
    "        # Boundary between sentence\n",
    "        step_result = evaluate_fn('O', 'O', last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts)\n",
    "        last_correct, last_correct_type, last_guessed, last_guessed_type, in_correct, counts = step_result\n",
    "        \n",
    "    if in_correct:\n",
    "        counts.correct_chunk += 1\n",
    "        counts.t_correct_chunk[last_correct_type] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "###\n",
    "# Calculate Metrics Function\n",
    "###\n",
    "def uniq(iterable):\n",
    "    seen = set()\n",
    "    return [i for i in iterable if not (i in seen or seen.add(i))]\n",
    "\n",
    "def calculate_metrics(correct, guessed, total):\n",
    "    tp, fp, fn = correct, guessed-correct, total-correct\n",
    "    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n",
    "    f = 0 if p + r == 0 else (2 * p * r) / (p + r)\n",
    "    return Metrics(tp, fp, fn, p, r, f)\n",
    "\n",
    "def metric(counts):\n",
    "    c = counts\n",
    "    overall = calculate_metrics(\n",
    "        c.correct_chunk, c.found_guessed, c.found_correct\n",
    "    )\n",
    "    by_type = {}\n",
    "    for t in uniq(list(c.t_found_correct.keys()) + list(c.t_found_guessed.keys())):\n",
    "        by_type[t] = calculate_metrics(\n",
    "            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n",
    "        )\n",
    "    return overall, by_type\n",
    "\n",
    "###\n",
    "# Main Function\n",
    "###\n",
    "def conll_evaluation(hyps_list, labels_list):\n",
    "    counts = evaluate(hyps_list, labels_list)\n",
    "    overall, by_type = metric(counts)\n",
    "\n",
    "    c = counts\n",
    "    acc = c.correct_tags / c.token_counter\n",
    "    pre = overall.prec\n",
    "    rec = overall.rec\n",
    "    f1 = overall.fscore\n",
    "    \n",
    "    type_macro_pre = 0.0\n",
    "    type_macro_rec = 0.0\n",
    "    type_macro_f1 = 0.0\n",
    "    for k in by_type.keys():\n",
    "        type_macro_pre += by_type[k].prec\n",
    "        type_macro_rec += by_type[k].rec\n",
    "        type_macro_f1 += by_type[k].fscore\n",
    "        \n",
    "    type_macro_pre = type_macro_pre / float(len(by_type))\n",
    "    type_macro_rec = type_macro_rec / float(len(by_type))\n",
    "    type_macro_f1 = type_macro_f1 / float(len(by_type))\n",
    "    \n",
    "    return (acc, pre, rec, f1, type_macro_pre, type_macro_rec, type_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_word_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 5:\n",
    "        (subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    subword_to_word_indices_batch = torch.LongTensor(subword_to_word_indices_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        subword_to_word_indices_batch = subword_to_word_indices_batch.cuda()\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(subword_batch, subword_to_word_indices_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
    "    loss, logits = outputs[:2]\n",
    "    \n",
    "    # generate prediction & label list\n",
    "    list_hyps = []\n",
    "    list_labels = []\n",
    "    hyps_list = torch.topk(logits, k=1, dim=-1)[1].squeeze(dim=-1)\n",
    "    for i in range(len(hyps_list)):\n",
    "        hyps, labels = hyps_list[i].tolist(), label_batch[i].tolist()        \n",
    "        list_hyp, list_label = [], []\n",
    "        for j in range(len(hyps)):\n",
    "            if labels[j] == -100:\n",
    "                break\n",
    "            else:\n",
    "                list_hyp.append(i2w[hyps[j]])\n",
    "                list_label.append(i2w[labels[j]])\n",
    "        list_hyps.append(list_hyp)\n",
    "        list_labels.append(list_label)\n",
    "        \n",
    "    return loss, list_hyps, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_metrics_fn(list_hyp, list_label):\n",
    "    metrics = {}\n",
    "    acc, pre, rec, f1, tm_pre, tm_rec, tm_f1 = conll_evaluation(list_hyp, list_label)\n",
    "    metrics[\"ACC\"] = acc\n",
    "    metrics[\"F1\"] = tm_f1\n",
    "    metrics[\"REC\"] = tm_rec\n",
    "    metrics[\"PRE\"] = tm_pre\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForWordClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        subword_to_word_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # average the token-level outputs to compute word-level representations\n",
    "        max_seq_len = subword_to_word_ids.max() + 1\n",
    "        word_latents = []\n",
    "        for i in range(max_seq_len):\n",
    "            mask = (subword_to_word_ids == i).unsqueeze(dim=-1)\n",
    "            word_latents.append((sequence_output * mask).sum(dim=1) / mask.sum())\n",
    "        word_batch = torch.stack(word_latents, dim=1)\n",
    "\n",
    "        sequence_output = self.dropout(word_batch)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTagProsaDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'B-PPO': 0, 'B-KUA': 1, 'B-ADV': 2, 'B-PRN': 3, 'B-VBI': 4, 'B-PAR': 5, 'B-VBP': 6, 'B-NNP': 7, 'B-UNS': 8, 'B-VBT': 9, 'B-VBL': 10, 'B-NNO': 11, 'B-ADJ': 12, 'B-PRR': 13, 'B-PRK': 14, 'B-CCN': 15, 'B-$$$': 16, 'B-ADK': 17, 'B-ART': 18, 'B-CSN': 19, 'B-NUM': 20, 'B-SYM': 21, 'B-INT': 22, 'B-NEG': 23, 'B-PRI': 24, 'B-VBE': 25}\n",
    "    INDEX2LABEL = {0: 'B-PPO', 1: 'B-KUA', 2: 'B-ADV', 3: 'B-PRN', 4: 'B-VBI', 5: 'B-PAR', 6: 'B-VBP', 7: 'B-NNP', 8: 'B-UNS', 9: 'B-VBT', 10: 'B-VBL', 11: 'B-NNO', 12: 'B-ADJ', 13: 'B-PRR', 14: 'B-PRK', 15: 'B-CCN', 16: 'B-$$$', 17: 'B-ADK', 18: 'B-ART', 19: 'B-CSN', 20: 'B-NUM', 21: 'B-SYM', 22: 'B-INT', 23: 'B-NEG', 24: 'B-PRI', 25: 'B-VBE'}\n",
    "    NUM_LABELS = 26\n",
    "    \n",
    "    def load_dataset(self, data):\n",
    "        # Prepare buffer\n",
    "        dataset = []\n",
    "        sentence = []\n",
    "        seq_label = []\n",
    "        for i in range (len(data)):\n",
    "            for j in range (len(data[i]['tokens'])):\n",
    "                sentence.append(data[i]['tokens'][j])\n",
    "                seq_label.append(self.LABEL2INDEX[data[i]['pos_tags'][j]])\n",
    "            dataset.append({\n",
    "                    'sentence': sentence,\n",
    "                    'seq_label': seq_label\n",
    "                })\n",
    "            sentence = []\n",
    "            seq_label = []\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        \n",
    "        # Add CLS token\n",
    "        subwords = [self.tokenizer.cls_token_id]\n",
    "        subword_to_word_indices = [-1] # For CLS\n",
    "        \n",
    "        # Add subwords\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add last SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        \n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label), data['sentence']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class PosTagDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(PosTagDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "\n",
    "        seq_list = []\n",
    "        for i, (subwords, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n",
    "\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"SEACrowd/posp\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config.num_labels = PosTagProsaDataset.NUM_LABELS\n",
    "w2i, i2w = PosTagProsaDataset.LABEL2INDEX, PosTagProsaDataset.INDEX2LABEL\n",
    "\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-large-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PosTagProsaDataset(data['train'], tokenizer, lowercase=True)\n",
    "valid_dataset = PosTagProsaDataset(data['validation'], tokenizer, lowercase=True)\n",
    "test_dataset = PosTagProsaDataset(data['test'], tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = PosTagDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=True)  \n",
    "valid_loader = PosTagDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=False)  \n",
    "test_loader = PosTagDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=8, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.8431 LR:0.00004000: 100%|██████████| 840/840 [01:19<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.8431 ACC:0.90 F1:0.78 REC:0.74 PRE:0.85 LR:0.00004000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4065 ACC:0.95 F1:0.91 REC:0.91 PRE:0.90: 100%|██████████| 105/105 [00:07<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.4065 ACC:0.95 F1:0.91 REC:0.91 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.2981 LR:0.00003600: 100%|██████████| 840/840 [01:19<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.2981 ACC:0.96 F1:0.93 REC:0.91 PRE:0.96 LR:0.00003600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2442 ACC:0.96 F1:0.95 REC:0.96 PRE:0.94: 100%|██████████| 105/105 [00:07<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.2442 ACC:0.96 F1:0.95 REC:0.96 PRE:0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.1670 LR:0.00003240: 100%|██████████| 840/840 [01:19<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.1670 ACC:0.98 F1:0.96 REC:0.96 PRE:0.97 LR:0.00003240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1951 ACC:0.96 F1:0.94 REC:0.95 PRE:0.96: 100%|██████████| 105/105 [00:07<00:00, 14.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.1951 ACC:0.96 F1:0.94 REC:0.95 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.1094 LR:0.00002916: 100%|██████████| 840/840 [01:19<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.1094 ACC:0.98 F1:0.97 REC:0.97 PRE:0.98 LR:0.00002916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1667 ACC:0.97 F1:0.97 REC:0.96 PRE:0.97: 100%|██████████| 105/105 [00:07<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.1667 ACC:0.97 F1:0.97 REC:0.96 PRE:0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0759 LR:0.00002624: 100%|██████████| 840/840 [01:19<00:00, 10.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0759 ACC:0.99 F1:0.98 REC:0.98 PRE:0.98 LR:0.00002624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1639 ACC:0.97 F1:0.96 REC:0.97 PRE:0.96: 100%|██████████| 105/105 [00:07<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.1639 ACC:0.97 F1:0.96 REC:0.97 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.0532 LR:0.00002362: 100%|██████████| 840/840 [01:19<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.0532 ACC:0.99 F1:0.99 REC:0.98 PRE:0.99 LR:0.00002362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1726 ACC:0.97 F1:0.97 REC:0.97 PRE:0.96: 100%|██████████| 105/105 [00:07<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.1726 ACC:0.97 F1:0.97 REC:0.97 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0408 LR:0.00002126: 100%|██████████| 840/840 [01:19<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0408 ACC:0.99 F1:0.99 REC:0.98 PRE:0.99 LR:0.00002126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1727 ACC:0.97 F1:0.96 REC:0.97 PRE:0.95: 100%|██████████| 105/105 [00:07<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.1727 ACC:0.97 F1:0.96 REC:0.97 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0304 LR:0.00001913: 100%|██████████| 840/840 [01:19<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0304 ACC:1.00 F1:0.99 REC:0.99 PRE:0.99 LR:0.00001913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1725 ACC:0.97 F1:0.96 REC:0.97 PRE:0.95: 100%|██████████| 105/105 [00:07<00:00, 14.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.1725 ACC:0.97 F1:0.96 REC:0.97 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.0229 LR:0.00001722: 100%|██████████| 840/840 [01:19<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.0229 ACC:1.00 F1:0.99 REC:0.99 PRE:1.00 LR:0.00001722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1817 ACC:0.97 F1:0.96 REC:0.97 PRE:0.96: 100%|██████████| 105/105 [00:07<00:00, 14.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.1817 ACC:0.97 F1:0.96 REC:0.97 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.0198 LR:0.00001550: 100%|██████████| 840/840 [01:19<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.0198 ACC:1.00 F1:0.99 REC:0.99 PRE:1.00 LR:0.00001550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1932 ACC:0.97 F1:0.96 REC:0.98 PRE:0.96: 100%|██████████| 105/105 [00:07<00:00, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.1932 ACC:0.97 F1:0.96 REC:0.98 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_norm = 10\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    " \n",
    "    total_train_loss = 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Calculate metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    # Calculate train metric\n",
    "    metrics = pos_tag_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate on validation\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        \n",
    "        # Calculate total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = pos_tag_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        \n",
    "    metrics = pos_tag_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:03<00:00, 28.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.97 F1:0.95 REC:0.95 PRE:0.96\n",
      "     index                                              label\n",
      "0        0  [B-NNO, B-NNO, B-NNP, B-VBI, B-PPO, B-NNP, B-S...\n",
      "1        1  [B-NNO, B-VBP, B-PPO, B-NNO, B-SYM, B-NNO, B-N...\n",
      "2        2  [B-SYM, B-NNO, B-PPO, B-NNO, B-VBI, B-PPO, B-N...\n",
      "3        3  [B-SYM, B-PRN, B-NNO, B-ART, B-NNO, B-NNO, B-S...\n",
      "4        4  [B-KUA, B-KUA, B-NNO, B-NNO, B-ADV, B-ADJ, B-C...\n",
      "..     ...                                                ...\n",
      "835    835  [B-NNP, B-ADV, B-VBT, B-NNO, B-NNP, B-PPO, B-V...\n",
      "836    836  [B-SYM, B-CCN, B-VBP, B-NNO, B-VBT, B-NNO, B-N...\n",
      "837    837  [B-NNP, B-NNP, B-NNO, B-ART, B-ADJ, B-VBP, B-P...\n",
      "838    838  [B-NNP, B-SYM, B-NNP, B-SYM, B-NNP, B-NNO, B-N...\n",
      "839    839  [B-NNO, B-NNO, B-NNP, B-SYM, B-NNP, B-SYM, B-N...\n",
      "\n",
      "[840 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):  \n",
    "    batch_seq = batch_data[-1]        \n",
    "    loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    list_hyp += batch_hyp\n",
    "    list_label += batch_label\n",
    "\n",
    "metrics = pos_tag_metrics_fn(list_hyp, list_label)\n",
    "print(metrics_to_string(metrics))\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('pred.txt', index=False)\n",
    "\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'postagger_indobert.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Pos Tagger Model for Verb Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Tokenize IndoSum Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum_data = load_dataset(\"maryantocinn/indosum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'ketua mpr zulkifli hasan menyesalkan kisruh yang terjadi antara pelaku sarana transportasi online dan tradisional',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 0},\n",
       " {'sentence': 'zulkifli menyarankan adanya pertemuan bersama antara pemerintah, pelaku transportasi online dan transportasi tradisional demi meredam kisruh yang masih belum terselesaikan',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 1},\n",
       " {'sentence': 'zulkifli menilai aturan yang dikeluarkan pemerintah seharusnya tidak hanya membahas tarif tapi juga mekanisme yang dapat menguntungkan semua pihak, baik pelaku transportasi online maupun tradisional',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 2},\n",
       " {'sentence': '\" tidak hanya tarif tapi apa saja harus diatur',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 3},\n",
       " {'sentence': 'dipanggil keduanya untuk berbicara masing-masing, musyawarah, duduk bareng kemudian dibuat aturan yang saling menguntungkan',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 4}]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_indosum = []\n",
    "sentence_data = []\n",
    "for i in range (len(indosum_data['train'])):\n",
    "    sentences = indosum_data['train'][i]['document'].split('. ')\n",
    "    for j in range (len(sentences)):\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', sentences[j].lower())\n",
    "        tokens.append('.')\n",
    "        pos_tags = ['B-NNP' for token in range (len(tokens))]\n",
    "        tokenized_indosum.append({'tokens': tokens, 'pos_tags': pos_tags})\n",
    "        sentence_data.append({'sentence': sentences[j].lower(), 'article_id': i, 'sentence_id': j})\n",
    "sentence_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'penulis: wisnu nova wistowo \\n                    \\n                                             editor: jalu wisnu wirajati \\n                                        \\n                                         sumber: \\n                                             bbc \\n                                        \\n                                    \\n            \\n            \\n           \\n                 topik: \\n                                \\n                                              \\n                             paul pogba \\n                        \\n                                              \\n                             manchester united \\n                        \\n                                              \\n                             premier league \\n                        \\n                                    \\n            \\n        \\n\\n        \\n                \\n        \\n            \\n                \\n                     komentar \\n                \\n                               \\n                  \\n                \\n            \\n        \\n    \\n\\n\\n\\n\\n     function send_counter () {\\n         var article_id = \" 177124 \"; \\n\\n         $.ajax ({\\n             url:\\' https://juara.bolasport.com/reads/send_counter\\', \\n             type:\\' post\\', \\n             datatype:\\' json\\', \\n             data: {article_id: article_id}, \\n        }) \\n             .done(function (data) {\\n                 if (data.status = = = true) {\\n                     console.log(\"success \"); \\n                } \\n\\n            }) \\n             .fail(function () {\\n                 console.log(\"error \"); \\n            }) \\n             .always(function () {\\n                 console.log(\"complete \"); \\n            }); \\n    } \\n\\n     window.onload = function () {\\n         send_counter (); \\n    }; \\n\\n     function fb_share(e, o) {\\n     return u = o, t = e, window.open(\"http://www.facebook.com / sharer.php?u= \" + encodeuricomponent(u) + \" & t= \" + encodeuricomponent(t), \" sharer \", \" toolbar=0,status=0,width=626,height=436 \"),! 1 \\n  } \\n\\n\\n            \\n            \\n            \\n                \\n\\t\\n        \\n\\t\\t   googletag.cmd.push(function () {googletag.display(\\'div-gpt-ad-143120217003854794-2-giant\\');}); \\n\\t\\t\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n    \\n      \\n       \\n\\n        \\n            \\n               googletag.cmd.push(function () {googletag.display(\\'div-gpt-ad-143120217003854794-4-right3\\');}); \\n            \\n        \\n\\n      \\n\\n      \\n        \\n         $(document).ready(function () {\\n         $(window).bind(\"load \", function () {\\n         $(\\' .sticky__rectangle\\').scrolltofixed ({\\n         margintop: 46, \\n         limit: $(\\' .sticky__rectangle\\').offset().top + 210, \\n         zindex: 11, \\n         unfixed: function () {\\n                         if ($(\\' .sticky__rectangle\\').hasclass(\"scroll - to - fixed - fixed \")) {\\n                             $(\\' .sticky__rectangle\\').css(\\'left\\',\\' \\'); \\n                             $(\\' .sticky__rectangle\\').css(\\'top\\',\\' \\'); \\n                        } \\n                    } \\n\\n        }); \\n        }); \\n        });',\n",
       " 'article_id': 14202,\n",
       " 'sentence_id': 8}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_indosum.pop(253116)\n",
    "sentence_data.pop(253116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indosum = PosTagProsaDataset(tokenized_indosum, tokenizer, lowercase=True)\n",
    "test_indosum_loader = PosTagDataLoader(dataset=test_indosum, max_seq_len=512, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Extract Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3152535/2882026469.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('postagger_indobert.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('postagger_indobert.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1643/1643 [00:57<00:00, 28.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                              label\n",
      "0          0  [B-NNO, B-NNP, B-NNP, B-NNP, B-VBT, B-ADJ, B-P...\n",
      "1          1  [B-NNP, B-VBT, B-NNO, B-NNO, B-VBI, B-PPO, B-N...\n",
      "2          2  [B-NNP, B-VBT, B-NNO, B-PRR, B-VBP, B-NNO, B-A...\n",
      "3          3  [B-SYM, B-NEG, B-ADV, B-NNO, B-CCN, B-PRI, B-A...\n",
      "4          4  [B-VBP, B-NUM, B-PPO, B-VBI, B-PRN, B-SYM, B-P...\n",
      "...      ...                                                ...\n",
      "13139  13139  [B-PPO, B-NNO, B-NUM, B-VBT, B-NNO, B-NNP, B-P...\n",
      "13140  13140  [B-SYM, B-CSN, B-SYM, B-PRN, B-VBP, B-VBT, B-N...\n",
      "13141  13141  [B-NNP, B-NNP, B-PAR, B-VBI, B-VBP, B-KUA, B-N...\n",
      "13142  13142  [B-SYM, B-CCN, B-PAR, B-PRN, B-ADV, B-VBT, B-P...\n",
      "13143  13143         [B-SYM, B-NNP, B-SYM, B-NNP, B-SYM, B-SYM]\n",
      "\n",
      "[13144 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "list_hyp, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_indosum_loader, leave=True, total=len(test_indosum_loader))\n",
    "for i, batch_data in enumerate(pbar):  \n",
    "    batch_seq = batch_data[-1]        \n",
    "    loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    list_hyp += batch_hyp\n",
    "    list_label += batch_label\n",
    "\n",
    "# Save prediction\n",
    "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "df.to_csv('indosum_postags_train.csv', index=False)\n",
    "\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'ketua mpr zulkifli hasan menyesalkan kisruh yang terjadi antara pelaku sarana transportasi online dan tradisional',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 0,\n",
       "  'verb': 'menyesalkan'},\n",
       " {'sentence': 'ketua mpr zulkifli hasan menyesalkan kisruh yang terjadi antara pelaku sarana transportasi online dan tradisional',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 0,\n",
       "  'verb': 'terjadi'},\n",
       " {'sentence': 'zulkifli menyarankan adanya pertemuan bersama antara pemerintah, pelaku transportasi online dan transportasi tradisional demi meredam kisruh yang masih belum terselesaikan',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 1,\n",
       "  'verb': 'menyarankan'},\n",
       " {'sentence': 'zulkifli menyarankan adanya pertemuan bersama antara pemerintah, pelaku transportasi online dan transportasi tradisional demi meredam kisruh yang masih belum terselesaikan',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 1,\n",
       "  'verb': 'bersama'},\n",
       " {'sentence': 'zulkifli menyarankan adanya pertemuan bersama antara pemerintah, pelaku transportasi online dan transportasi tradisional demi meredam kisruh yang masih belum terselesaikan',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 1,\n",
       "  'verb': 'meredam'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_postag = []\n",
    "for i in range (len(sentence_data)):\n",
    "    for j in range (len(df.loc[i]['label'])):\n",
    "        if w2i[df.loc[i]['label'][j]] in [4,6,9,10,25]:\n",
    "            sentence_postag.append({'sentence': sentence_data[i]['sentence'], 'article_id': sentence_data[i]['article_id'], 'sentence_id': sentence_data[i]['sentence_id'], 'verb': tokenized_indosum[i]['tokens'][j]})\n",
    "sentence_postag[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ketua mpr zulkifli hasan menyesalkan kisruh ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>menyesalkan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ketua mpr zulkifli hasan menyesalkan kisruh ya...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>terjadi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zulkifli menyarankan adanya pertemuan bersama ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>menyarankan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zulkifli menyarankan adanya pertemuan bersama ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bersama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zulkifli menyarankan adanya pertemuan bersama ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>meredam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  article_id  sentence_id  \\\n",
       "0  ketua mpr zulkifli hasan menyesalkan kisruh ya...           0            0   \n",
       "1  ketua mpr zulkifli hasan menyesalkan kisruh ya...           0            0   \n",
       "2  zulkifli menyarankan adanya pertemuan bersama ...           0            1   \n",
       "3  zulkifli menyarankan adanya pertemuan bersama ...           0            1   \n",
       "4  zulkifli menyarankan adanya pertemuan bersama ...           0            1   \n",
       "\n",
       "          verb  \n",
       "0  menyesalkan  \n",
       "1      terjadi  \n",
       "2  menyarankan  \n",
       "3      bersama  \n",
       "4      meredam  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_df = pd.DataFrame(sentence_postag)\n",
    "verb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_df.to_csv('indosum_verb_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
