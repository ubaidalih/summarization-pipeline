{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 15:38:54.759927: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 15:38:54.759987: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 15:38:54.760002: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 15:38:54.766408: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 15:38:55.599574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModelForTokenClassification, BertPreTrainedModel, BertConfig\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import re\n",
    "\n",
    "# Suppress SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForWordClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        subword_to_word_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # average the token-level outputs to compute word-level representations\n",
    "        max_seq_len = subword_to_word_ids.max() + 1\n",
    "        word_latents = []\n",
    "        for i in range(max_seq_len):\n",
    "            mask = (subword_to_word_ids == i).unsqueeze(dim=-1)\n",
    "            word_latents.append((sequence_output * mask).sum(dim=1) / mask.sum())\n",
    "        word_batch = torch.stack(word_latents, dim=1)\n",
    "\n",
    "        sequence_output = self.dropout(word_batch)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_word_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 5:\n",
    "        (subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    subword_to_word_indices_batch = torch.LongTensor(subword_to_word_indices_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        subword_to_word_indices_batch = subword_to_word_indices_batch.cuda()\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(subword_batch, subword_to_word_indices_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
    "    loss, logits = outputs[:2]\n",
    "    \n",
    "    # generate prediction & label list\n",
    "    list_hyps = []\n",
    "    list_labels = []\n",
    "    hyps_list = torch.topk(logits, k=1, dim=-1)[1].squeeze(dim=-1)\n",
    "    for i in range(len(hyps_list)):\n",
    "        hyps, labels = hyps_list[i].tolist(), label_batch[i].tolist()        \n",
    "        list_hyp, list_label = [], []\n",
    "        for j in range(len(hyps)):\n",
    "            if labels[j] == -100:\n",
    "                break\n",
    "            else:\n",
    "                list_hyp.append(i2w[hyps[j]])\n",
    "                list_label.append(i2w[labels[j]])\n",
    "        list_hyps.append(list_hyp)\n",
    "        list_labels.append(list_label)\n",
    "        \n",
    "    return loss, list_hyps, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTagProsaDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'B-PPO': 0, 'B-KUA': 1, 'B-ADV': 2, 'B-PRN': 3, 'B-VBI': 4, 'B-PAR': 5, 'B-VBP': 6, 'B-NNP': 7, 'B-UNS': 8, 'B-VBT': 9, 'B-VBL': 10, 'B-NNO': 11, 'B-ADJ': 12, 'B-PRR': 13, 'B-PRK': 14, 'B-CCN': 15, 'B-$$$': 16, 'B-ADK': 17, 'B-ART': 18, 'B-CSN': 19, 'B-NUM': 20, 'B-SYM': 21, 'B-INT': 22, 'B-NEG': 23, 'B-PRI': 24, 'B-VBE': 25}\n",
    "    INDEX2LABEL = {0: 'B-PPO', 1: 'B-KUA', 2: 'B-ADV', 3: 'B-PRN', 4: 'B-VBI', 5: 'B-PAR', 6: 'B-VBP', 7: 'B-NNP', 8: 'B-UNS', 9: 'B-VBT', 10: 'B-VBL', 11: 'B-NNO', 12: 'B-ADJ', 13: 'B-PRR', 14: 'B-PRK', 15: 'B-CCN', 16: 'B-$$$', 17: 'B-ADK', 18: 'B-ART', 19: 'B-CSN', 20: 'B-NUM', 21: 'B-SYM', 22: 'B-INT', 23: 'B-NEG', 24: 'B-PRI', 25: 'B-VBE'}\n",
    "    NUM_LABELS = 26\n",
    "    \n",
    "    def load_dataset(self, data):\n",
    "        # Prepare buffer\n",
    "        dataset = []\n",
    "        sentence = []\n",
    "        seq_label = []\n",
    "        for i in range (len(data)):\n",
    "            for j in range (len(data[i]['tokens'])):\n",
    "                sentence.append(data[i]['tokens'][j])\n",
    "                seq_label.append(self.LABEL2INDEX[data[i]['pos_tags'][j]])\n",
    "            dataset.append({\n",
    "                    'sentence': sentence,\n",
    "                    'seq_label': seq_label\n",
    "                })\n",
    "            sentence = []\n",
    "            seq_label = []\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        \n",
    "        # Add CLS token\n",
    "        subwords = [self.tokenizer.cls_token_id]\n",
    "        subword_to_word_indices = [-1] # For CLS\n",
    "        \n",
    "        # Add subwords\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add last SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        \n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label), data['sentence']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTagDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(PosTagDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "\n",
    "        seq_list = []\n",
    "        for i, (subwords, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n",
    "\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagModel():\n",
    "    def __init__(self, model_dir):\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "    def preprocess_text(self, data):\n",
    "        tokenized_data = []\n",
    "        sentence_data = []\n",
    "        sentences = data.split('. ')\n",
    "        for i in range (len(sentences)):\n",
    "            tokens = re.findall(r'\\w+|[^\\w\\s]', sentences[i].lower())\n",
    "            tokens.append('.')\n",
    "            pos_tags = ['B-NNP' for token in range (len(tokens))]\n",
    "            tokenized_data.append({'tokens': tokens, 'pos_tags': pos_tags})\n",
    "            sentence_data.append({'sentence': \" \".join(tokens), 'sentence_id': i})\n",
    "        return tokenized_data, sentence_data\n",
    "\n",
    "    def verb_extraction(self, data_text):\n",
    "        tokenized_data, sentence_data = self.preprocess_text(data_text)\n",
    "        tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "        data = PosTagProsaDataset(tokenized_data, tokenizer, lowercase=True)\n",
    "        data_loader = PosTagDataLoader(dataset=data, max_seq_len=512, batch_size=8, shuffle=False)\n",
    "\n",
    "        config = BertConfig.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "        config.num_labels = PosTagProsaDataset.NUM_LABELS\n",
    "        w2i, i2w = PosTagProsaDataset.LABEL2INDEX, PosTagProsaDataset.INDEX2LABEL\n",
    "        model = BertForWordClassification.from_pretrained('indobenchmark/indobert-large-p1', config=config)\n",
    "        model.cuda()\n",
    "        model.load_state_dict(torch.load(self.model_dir))\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        list_hyp, list_label = [], []\n",
    "\n",
    "        for batch_data in data_loader:     \n",
    "            loss, batch_hyp, batch_label = forward_word_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "\n",
    "        # Save prediction\n",
    "        df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "        sentence_postag = []\n",
    "        for i in range (len(sentence_data)):\n",
    "            for j in range (len(df.loc[i]['label'])):\n",
    "                if w2i[df.loc[i]['label'][j]] in [4,6,9,10,25]:\n",
    "                    sentence_postag.append({'sentence': sentence_data[i]['sentence'], 'sentence_id': sentence_data[i]['sentence_id'], 'verb': tokenized_data[i]['tokens'][j]})\n",
    "        verb_df = pd.DataFrame(sentence_postag)\n",
    "        return verb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"default\": {\n",
    "        \"srl_labels\": [\"ARG0\",\"ARG1\",\"ARG2\",\"AM-MOD\",\"AM-ADV\",\"REL\",\"AM-TMP\",\"AM-CAU\",\"AM-LOC\",\"AM-DIR\", \"AM-MNR\",\"AM-DIS\",\"AM-PRD\",\"ARG3\",\"ARG4\",\"AM-LVB\",\"AM-PRP\",\"AM-COM\",\"AM-GOL\",\"AM-EXT\",\"AM-REC\",\"AM-NEG\",\"AM-ADJ\"],\n",
    "        \"max_len\": 512,\n",
    "    },\n",
    "    \"xlmr\": {\n",
    "      \"model\": \"FacebookAI/xlm-roberta-large\",\n",
    "      \"batch_size\": 32,\n",
    "      \"model_location\": \"model/srl\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSRLModel():\n",
    "    def __init__(self, config):\n",
    "        self.name = config[\"model\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer: AutoTokenizer = None\n",
    "        self.model: AutoModelForTokenClassification = None\n",
    "\n",
    "    def build_label_vocab(self) -> dict:\n",
    "        label2index = {\"O\": 0}\n",
    "        for j in range(2):\n",
    "          tag = \"B-\" if j==0 else \"I-\"\n",
    "          for labelset in configurations[\"default\"][\"srl_labels\"]:\n",
    "            if labelset not in label2index:\n",
    "              label2index[tag+labelset] = len(label2index)\n",
    "        return label2index\n",
    "\n",
    "    def load_model(self, model_dir):\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def expand_label_token(self, original_sentence, original_labels):\n",
    "        tmp_labels = []\n",
    "        txt_sentences = \" \".join(original_sentence)\n",
    "        tokens = self.tokenizer(txt_sentences, padding=\"max_length\", truncation=True, max_length=configurations[\"default\"][\"max_len\"])\n",
    "        sentence = self.tokenizer.tokenize(txt_sentences)\n",
    "        for i, word in enumerate(original_sentence):\n",
    "            word_pieces = self.tokenizer.tokenize(word)\n",
    "            if len(word_pieces) == 1:\n",
    "                tmp_labels.append(original_labels[i])\n",
    "            else:\n",
    "                tmp_labels.append(original_labels[i])\n",
    "                for _ in range (1, len(word_pieces)):\n",
    "                    if original_labels[i] == \"O\":\n",
    "                        tmp_labels.append(\"O\")\n",
    "                    else:\n",
    "                        tmp_labels.append(\"I\" + original_labels[i][1:])\n",
    "        labels = [\"O\"] + tmp_labels + [\"O\"]\n",
    "        length = len(sentence)+2\n",
    "        return tokens, labels, length\n",
    "\n",
    "    def get_data(self, data):\n",
    "        list_input_ids, verb_indicators, all_labels, attention_masks, length_sentences = [], [], [], [], []\n",
    "        for _, obj in enumerate(data):\n",
    "            tokens, labelset, length_sentence = self.expand_label_token(obj[\"words\"], obj[\"arguments\"])\n",
    "            input_ids = tokens[\"input_ids\"]\n",
    "            attent_mask = tokens[\"attention_mask\"]\n",
    "            list_input_ids.append(input_ids)\n",
    "            attention_masks.append(attent_mask)\n",
    "            length_sentences.append(length_sentence)\n",
    "            # Verb Indicator (which predicate to label)\n",
    "            bio_verb = [1 if label[2:] == \"REL\" else 0 for label in labelset]\n",
    "            verb_indicators.append(bio_verb)\n",
    "            all_labels.append(labelset)\n",
    "\n",
    "        return list_input_ids, verb_indicators, all_labels, attention_masks, length_sentences\n",
    "\n",
    "    def load_srl_dataset(self, data):\n",
    "        input_ids, verb_indicators, labels, attention_masks, seq_lengths = self.get_data(data)\n",
    "        label_ixs = []\n",
    "        label2index = self.build_label_vocab()\n",
    "        # Convert label to their indices\n",
    "        for i, labelset in enumerate(labels):\n",
    "            label_ixs.append([label2index.get(l, 1) for l in labelset])\n",
    "        # pad label and verb consequence\n",
    "        input_is_pred = pad_sequences(verb_indicators, maxlen=configurations[\"default\"][\"max_len\"], dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        label_ids = pad_sequences(label_ixs, maxlen=configurations[\"default\"][\"max_len\"], dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        label_ids = torch.LongTensor(label_ids)\n",
    "        return torch.LongTensor(input_ids), torch.LongTensor(attention_masks), label_ids,  torch.LongTensor(seq_lengths), torch.LongTensor(input_is_pred)\n",
    "\n",
    "    def create_pred_dataloader(self, data):\n",
    "        prediction_inputs, prediction_masks, gold_labels, seq_lens, gold_predicates = self.load_srl_dataset(data)\n",
    "        # Create the DataLoader.\n",
    "        prediction_data = TensorDataset(prediction_inputs, prediction_masks, gold_labels, seq_lens, gold_predicates)\n",
    "        prediction_sampler = SequentialSampler(prediction_data)\n",
    "        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=self.batch_size)\n",
    "        return prediction_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRLModel():\n",
    "    def __init__(self, config):\n",
    "        self.model_dir = config[\"model_location\"]\n",
    "        self.transformer_model: TransformerSRLModel = TransformerSRLModel(config)\n",
    "\n",
    "    def preprocess_text(self,data):\n",
    "        list_data = []\n",
    "        for i, text in data.iterrows():\n",
    "            words = text[\"sentence\"].split(\" \")\n",
    "            labels = [\"O\" for i in range (len(words))]\n",
    "            index = words.index(text[\"verb\"])\n",
    "            labels[index] = \"B-REL\"\n",
    "            srl = {\n",
    "            \"words\" : words,\n",
    "            \"arguments\" : labels,\n",
    "            \"predicate\" : text[\"verb\"]\n",
    "            }\n",
    "            list_data.append(srl)\n",
    "        return list_data\n",
    "    \n",
    "    def filter_label(self, text, gold_labels, pred_labels):\n",
    "        new_gold_labels, new_pred_labels = [], []\n",
    "        for i in range(len(text)):\n",
    "            if text[i].startswith(\"▁\"):\n",
    "                new_gold_labels.append(gold_labels[i])\n",
    "                new_pred_labels.append(pred_labels[i])\n",
    "            else:\n",
    "                continue\n",
    "        return new_gold_labels, new_pred_labels\n",
    "\n",
    "    def generate_label(self, data_text):\n",
    "        data = self.preprocess_text(data_text)\n",
    "        # Load Saved Model\n",
    "        self.transformer_model.load_model(self.model_dir)\n",
    "        model = self.transformer_model.model\n",
    "        tokenizer =self.transformer_model.tokenizer\n",
    "        label2index = self.transformer_model.build_label_vocab()\n",
    "        index2label = {v: k for k, v in label2index.items()}\n",
    "        # Load File for Predictions\n",
    "        pred_dataloader = self.transformer_model.create_pred_dataloader(data)\n",
    "        model.eval()\n",
    "        total_sents = 0\n",
    "        list_pred_labels = []\n",
    "\n",
    "\n",
    "        for batch in pred_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(self.transformer_model.device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels, b_lengths, b_preds = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=b_preds, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            class_probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Move class_probabilities and labels to CPU\n",
    "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
    "            argmax_indices = np.argmax(class_probabilities, axis=-1)\n",
    "\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            seq_lengths = b_lengths.to('cpu').numpy()\n",
    "\n",
    "            for ix in range(len(label_ids)):\n",
    "                total_sents += 1\n",
    "                text = tokenizer.convert_ids_to_tokens(b_input_ids[ix])\n",
    "                # Store predictions and true labels\n",
    "                pred_labels = [index2label[p] for p in argmax_indices[ix][:seq_lengths[ix]]]\n",
    "                gold_labels = [index2label[g] for g in label_ids[ix]]\n",
    "                # Delete unnecessary label\n",
    "                idx_pad = text.index(\"<pad>\")\n",
    "                pred_labels = pred_labels[1:len(pred_labels)-1]\n",
    "                gold_labels = gold_labels[1:idx_pad-1]\n",
    "                text = text[1:idx_pad-1]\n",
    "                _, pred_labels = self.filter_label(text, gold_labels, pred_labels)\n",
    "                list_pred_labels.append(pred_labels)\n",
    "        return list_pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationModel():\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "        self.model = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def get_word_embeddings(self, tokens):\n",
    "        inputs = self.tokenizer(tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "    def filter_labels_if_not_in_common(self, embed1, embed2, labels1, labels2):\n",
    "        common_labels = set(labels1).intersection(set(labels2))\n",
    "\n",
    "        embed1 = [embed for embed, label in zip(embed1, labels1) if label in common_labels]\n",
    "        label1 = [label for label in labels1 if label in common_labels]\n",
    "        embed2 = [embed for embed, label in zip(embed2, labels2) if label in common_labels]\n",
    "        label2 = [label for label in labels2 if label in common_labels]\n",
    "\n",
    "        all_elements = set(labels1).union(set(labels2))\n",
    "\n",
    "        return embed1, embed2, label1, label2, len(all_elements)\n",
    "\n",
    "    def filter_words_labels(self, srl_tag):\n",
    "        filtered_labels = [label for label in srl_tag if label != 'O']\n",
    "        filtered_labels = [label[2:] for label in filtered_labels]\n",
    "        return filtered_labels\n",
    "\n",
    "    def filter_words_token(self, row):\n",
    "        tokens = row[\"sentence\"].split()\n",
    "        labels = row[\"srl\"]\n",
    "        filtered_tokens = [token for token, label in zip(tokens, labels) if label != 'O']\n",
    "        return filtered_tokens\n",
    "\n",
    "    def sentence_similarity(self, embeddings1, embeddings2, labels1, labels2, count):\n",
    "        max_similarities = {}\n",
    "\n",
    "        for i, (emb1, label1) in enumerate(zip(embeddings1, labels1)):\n",
    "            for j, (emb2, label2) in enumerate(zip(embeddings2, labels2)):\n",
    "                if label1 == label2:\n",
    "                    # Calculate similarity\n",
    "                    similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "                    if label1 not in max_similarities or similarity > max_similarities[label1]:\n",
    "                        max_similarities[label1] = similarity\n",
    "\n",
    "        # Sum up the maximum similarities for each label\n",
    "        total_max_similarity = sum(max_similarities.values())\n",
    "\n",
    "        return total_max_similarity / count\n",
    "\n",
    "    def count_o_labels(self, srl_tags):\n",
    "        return srl_tags.count('O')\n",
    "\n",
    "    def reduce_same_sentence(self, data):\n",
    "        data['o_label_count'] = data['srl'].apply(self.count_o_labels)\n",
    "        data_sorted = data.sort_values(by='o_label_count', ascending=True)\n",
    "        data_reduced = data_sorted.drop_duplicates(subset='sentence', keep='first')\n",
    "        return data_reduced.sort_index()\n",
    "\n",
    "    def calculate_sentence_scores(self, data):\n",
    "        sentence_scores = []\n",
    "        similarity_cache = {}  # Cache to store previously computed similarities\n",
    "        for i, row1 in data.iterrows():\n",
    "            embeddings1, labels1 = row1['embeddings'], row1['srl']\n",
    "            sentence_score = 0\n",
    "            \n",
    "            for j, row2 in data.iterrows():\n",
    "                if i != j:\n",
    "                    # Check if this similarity has been computed before\n",
    "                    if (i, j) in similarity_cache:\n",
    "                        similarity = similarity_cache[(i, j)]\n",
    "                    elif (j, i) in similarity_cache:\n",
    "                        similarity = similarity_cache[(j, i)]\n",
    "                    else:\n",
    "                        embeddings2, labels2 = row2['embeddings'], row2['srl']\n",
    "                        embeddings1, embeddings2, labels1, labels2, count = self.filter_labels_if_not_in_common(embeddings1, embeddings2, labels1, labels2)\n",
    "                        similarity = self.sentence_similarity(embeddings1, embeddings2, labels1, labels2, count)\n",
    "                        similarity_cache[(i, j)] = similarity\n",
    "                        similarity_cache[(j, i)] = similarity\n",
    "                    \n",
    "                    sentence_score += similarity\n",
    "            \n",
    "            sentence_scores.append(sentence_score)\n",
    "        \n",
    "        return sentence_scores\n",
    "    \n",
    "    def summarize_data(self, data):\n",
    "        data = self.reduce_same_sentence(data)\n",
    "        data[\"labels\"] =  data['srl'].apply(self.filter_words_labels)\n",
    "        data[\"token\"] = data.apply(self.filter_words_token, axis=1)\n",
    "        data['embeddings'] = data[\"token\"].apply(self.get_word_embeddings)\n",
    "        score = self.calculate_sentence_scores(data)\n",
    "        data[\"score\"] = score\n",
    "        data.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "        top_count = math.ceil(len(data) / 4)\n",
    "        top = data.head(top_count)\n",
    "        top.sort_values(\"sentence_id\", inplace=True)\n",
    "        sentences = \"\"\n",
    "        for i, sentence in top.iterrows():\n",
    "            sentences += \" \" + sentence[\"sentence\"]\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2260667/1682577407.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(self.model_dir))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pendidikan bertujuan untuk menggali dan memanfaatkan potensi keunikan individu dan menjadikannya berguna bagi diri sendiri dan lingkungan . hal ini juga berarti bahwa pendidikan membantu manusia menemukan potensi dan bakatnya sendiri , serta mengembangkannya sesuai dengan keunikan dan keahliannya .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summary_article(article):\n",
    "    # pos tag model\n",
    "    postag_model = POSTagModel('model/postag/postagger_indobert.pth')\n",
    "    data = postag_model.verb_extraction(article)\n",
    "    \n",
    "    #srl model\n",
    "    model_predict = SRLModel(configurations[\"xlmr\"])\n",
    "    list_srl_label = model_predict.generate_label(data)\n",
    "\n",
    "    # summarization model\n",
    "    summarization_model = SummarizationModel()\n",
    "    data[\"srl\"] = list_srl_label\n",
    "    summary = summarization_model.summarize_data(data)\n",
    "    return summary.strip()\n",
    "\n",
    "article = (\n",
    "    \"Pendidikan merupakan pilar utama dalam pembangunan suatu bangsa. Pendidikan dipahami \"\n",
    "    \"secara luas sebagai proses belajar terus menerus sepanjang hayat, maka pendidikan menjadi \"\n",
    "    \"komponen penting. Melalui pengalaman hidup sehari-hari, proses ini alami, langsung atau \"\n",
    "    \"tidak langsung. \"\n",
    "    \"Pendidikan bertujuan untuk menggali dan memanfaatkan potensi keunikan individu dan \"\n",
    "    \"menjadikannya berguna bagi diri sendiri dan lingkungan. Hal ini juga berarti bahwa pendidikan \"\n",
    "    \"membantu manusia menemukan potensi dan bakatnya sendiri, serta mengembangkannya \"\n",
    "    \"sesuai dengan keunikan dan keahliannya. Oleh karena itu, dapat dikatakan bahwa pendidikan\"\n",
    "    \"adalah hak setiap orang. \"\n",
    "    \"Pendidikan tidak hanya sebatas belajar di sekolah. Demikian pula sistem pendidikan tidak \"\n",
    "    \"hanya eksis dalam bentuk formal yang dikenal dan berkembang di masyarakat. Artikel ini akan \"\n",
    "    \"membahas mengenai pentingnya pendidikan dalam membentuk masyarakat yang cerdas, \"\n",
    "    \"produktif, dan berdaya saing, serta peranannya dalam menciptakan masa depan yang lebih \"\n",
    "    \"baik bagi generasi mendatang.\"\n",
    ")\n",
    "\n",
    "summary_article(article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
