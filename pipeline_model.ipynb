{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:38:47.513459: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-12 12:38:47.513523: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-12 12:38:47.513547: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-12 12:38:47.519847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-12 12:38:48.281445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertConfig, BertTokenizer, BertPreTrainedModel, BertModel, AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_dataset\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from datasets import load_dataset\n",
    "from tabulate import tabulate\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pos Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForWordClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        subword_to_word_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # average the token-level outputs to compute word-level representations\n",
    "        max_seq_len = subword_to_word_ids.max() + 1\n",
    "        word_latents = []\n",
    "        for i in range(max_seq_len):\n",
    "            mask = (subword_to_word_ids == i).unsqueeze(dim=-1)\n",
    "            word_latents.append((sequence_output * mask).sum(dim=1) / mask.sum())\n",
    "        word_batch = torch.stack(word_latents, dim=1)\n",
    "\n",
    "        sequence_output = self.dropout(word_batch)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), scores, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosTagProsaDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'B-PPO': 0, 'B-KUA': 1, 'B-ADV': 2, 'B-PRN': 3, 'B-VBI': 4, 'B-PAR': 5, 'B-VBP': 6, 'B-NNP': 7, 'B-UNS': 8, 'B-VBT': 9, 'B-VBL': 10, 'B-NNO': 11, 'B-ADJ': 12, 'B-PRR': 13, 'B-PRK': 14, 'B-CCN': 15, 'B-$$$': 16, 'B-ADK': 17, 'B-ART': 18, 'B-CSN': 19, 'B-NUM': 20, 'B-SYM': 21, 'B-INT': 22, 'B-NEG': 23, 'B-PRI': 24, 'B-VBE': 25}\n",
    "    INDEX2LABEL = {0: 'B-PPO', 1: 'B-KUA', 2: 'B-ADV', 3: 'B-PRN', 4: 'B-VBI', 5: 'B-PAR', 6: 'B-VBP', 7: 'B-NNP', 8: 'B-UNS', 9: 'B-VBT', 10: 'B-VBL', 11: 'B-NNO', 12: 'B-ADJ', 13: 'B-PRR', 14: 'B-PRK', 15: 'B-CCN', 16: 'B-$$$', 17: 'B-ADK', 18: 'B-ART', 19: 'B-CSN', 20: 'B-NUM', 21: 'B-SYM', 22: 'B-INT', 23: 'B-NEG', 24: 'B-PRI', 25: 'B-VBE'}\n",
    "    NUM_LABELS = 26\n",
    "    \n",
    "    def load_dataset(self, data):\n",
    "        # Prepare buffer\n",
    "        dataset = []\n",
    "        sentence = []\n",
    "        seq_label = []\n",
    "        for i in range (len(data)):\n",
    "            for j in range (len(data[i]['tokens'])):\n",
    "                sentence.append(data[i]['tokens'][j])\n",
    "                seq_label.append(self.LABEL2INDEX[data[i]['pos_tags'][j]])\n",
    "            dataset.append({\n",
    "                    'sentence': sentence,\n",
    "                    'seq_label': seq_label\n",
    "                })\n",
    "            sentence = []\n",
    "            seq_label = []\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        \n",
    "        # Add CLS token\n",
    "        subwords = [self.tokenizer.cls_token_id]\n",
    "        subword_to_word_indices = [-1] # For CLS\n",
    "        \n",
    "        # Add subwords\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add last SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        \n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label), data['sentence']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class PosTagDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(PosTagDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "\n",
    "        seq_list = []\n",
    "        for i, (subwords, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n",
    "\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, subword_to_word_indices_batch, seq_label_batch, seq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "indosum_data = load_dataset(\"maryantocinn/indosum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'jakarta, cnn indonesia - - dokter ryan thamrin, yang terkenal lewat acara dokter oz indonesia, meninggal dunia pada jumat (4 / 8) dini hari',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 0},\n",
       " {'sentence': 'dokter lula kamal yang merupakan selebriti sekaligus rekan kerja ryan menyebut kawannya itu sudah sakit sejak setahun yang lalu',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 1},\n",
       " {'sentence': 'lula menuturkan, sakit itu membuat ryan mesti vakum dari semua kegiatannya, termasuk menjadi pembawa acara dokter oz indonesia',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 2},\n",
       " {'sentence': 'kondisi itu membuat ryan harus kembali ke kampung halamannya di pekanbaru, riau untuk menjalani istirahat',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 3},\n",
       " {'sentence': '\" setahu saya dia orangnya sehat, tapi tahun lalu saya dengar dia sakit',\n",
       "  'article_id': 0,\n",
       "  'sentence_id': 4}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_indosum = []\n",
    "sentence_data = []\n",
    "for i in range (len(indosum_data['train'])):\n",
    "    sentences = indosum_data['train'][i]['document'].split('. ')\n",
    "    for j in range (len(sentences)):\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', sentences[j].lower())\n",
    "        tokens.append('.')\n",
    "        pos_tags = ['B-NNP' for token in range (len(tokens))]\n",
    "        tokenized_indosum.append({'tokens': tokens, 'pos_tags': pos_tags})\n",
    "        sentence_data.append({'sentence': sentences[j].lower(), 'article_id': i, 'sentence_id': j})\n",
    "sentence_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-large-p1')\n",
    "config.num_labels = PosTagProsaDataset.NUM_LABELS\n",
    "w2i, i2w = PosTagProsaDataset.LABEL2INDEX, PosTagProsaDataset.INDEX2LABEL\n",
    "\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-large-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1278007/3623210521.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model/postag/postagger_indobert.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d822d407254b18823eef77e0bd5762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_indosum = PosTagProsaDataset(tokenized_indosum, tokenizer, lowercase=True)\n",
    "test_indosum_loader = PosTagDataLoader(dataset=test_indosum, max_seq_len=512, batch_size=8, shuffle=False)\n",
    "model.load_state_dict(torch.load('model/postag/postagger_indobert.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Role Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"default\": {\n",
    "        \"srl_labels\": [\"ARG0\",\"ARG1\",\"ARG2\",\"AM-MOD\",\"AM-ADV\",\"REL\",\"AM-TMP\",\"AM-CAU\",\"AM-LOC\",\"AM-DIR\", \"AM-MNR\",\"AM-DIS\",\"AM-PRD\",\"ARG3\",\"ARG4\",\"AM-LVB\",\"AM-PRP\",\"AM-COM\",\"AM-GOL\",\"AM-EXT\",\"AM-REC\",\"AM-NEG\",\"AM-ADJ\"],\n",
    "        \"verb_labels\": [\"B-VBE\", \"B-VBI\", \"B-VBL\", \"B-VBP\", \"B-VBT\"],\n",
    "        \"seed_val\": 43,\n",
    "        \"max_len\": 512,\n",
    "        \"info_every\": 30,\n",
    "        \"gradient_clip\": 1.0,\n",
    "        \"gozali_data_location\": \"data/srl_corpus_gojali.txt\"\n",
    "    },\n",
    "    \"xlmr_32\": {\n",
    "      \"model\": \"FacebookAI/xlm-roberta-large\",\n",
    "      \"batch_size\": 32,\n",
    "      \"epochs\": 8,\n",
    "      \"learning_rate\": 1e-4,\n",
    "      \"model_location\": \"model/srl\",\n",
    "      \"load_model_location\": \"model/srl\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = configurations[\"default\"][\"seed_val\"]\n",
    "device = \"cuda:7\"\n",
    "LongTensor = torch.LongTensor\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixModel():\n",
    "    def __init__(self):\n",
    "        self.arg_excess = defaultdict(int)\n",
    "        self.arg_missed = defaultdict(int)\n",
    "        self.arg_match = defaultdict(int)\n",
    "\n",
    "    def filter_label(self, text, gold_labels, pred_labels):\n",
    "        new_gold_labels, new_pred_labels = [], []\n",
    "        for i in range(len(text)):\n",
    "            if text[i].startswith(\"▁\"):\n",
    "                new_gold_labels.append(gold_labels[i])\n",
    "                new_pred_labels.append(pred_labels[i])\n",
    "            else:\n",
    "                continue\n",
    "        return new_gold_labels, new_pred_labels\n",
    "\n",
    "    def evaluate_tagset(self, text, gold_labels, pred_labels):\n",
    "        new_gold_labels, new_pred_labels = self.filter_label(text, gold_labels, pred_labels)\n",
    "        label_filter = [\"O\"]\n",
    "        gld = set([f\"{i}_{y}\" for i, y in enumerate(new_gold_labels) if y not in label_filter])\n",
    "        sys = set([f\"{i}_{y}\" for i, y in enumerate(new_pred_labels) if y not in label_filter])\n",
    "\n",
    "        excess = sys - gld  # False Positives\n",
    "        missed = gld - sys  # False Negatives\n",
    "        true_pos = sys.intersection(gld)\n",
    "\n",
    "        eval_obj = {\"excess\": [x.split(\"_\")[1][2:] for x in excess],\n",
    "                    \"missed\": [x.split(\"_\")[1][2:] for x in missed],\n",
    "                    \"match\": [x.split(\"_\")[1][2:] for x in true_pos]}\n",
    "        self.add_to_eval_dicts(eval_obj)\n",
    "\n",
    "    def add_to_eval_dicts(self, eval_metrics):\n",
    "        for arg in eval_metrics[\"excess\"]:\n",
    "            self.arg_excess[arg] += 1\n",
    "        for arg in eval_metrics[\"missed\"]:\n",
    "            self.arg_missed[arg] += 1\n",
    "        for arg in eval_metrics[\"match\"]:\n",
    "            self.arg_match[arg] += 1\n",
    "\n",
    "    def get_metrics(self, false_pos, false_neg, true_pos):\n",
    "        _denom1 = true_pos + false_pos\n",
    "        precision = true_pos / _denom1 if _denom1 else 0\n",
    "        _denom2 = true_pos + false_neg\n",
    "        recall = true_pos / _denom2 if _denom2 else 0\n",
    "        _denom3 = precision + recall\n",
    "        F1 = 2 * ((precision * recall) / _denom3) if _denom3 else 0\n",
    "        return precision*100, recall*100, F1*100\n",
    "\n",
    "    def show_overall_metrics(self, save_to_file=None, print_metrics=True):\n",
    "        processed_args = set()\n",
    "        results = []\n",
    "        tot_excess, tot_missed, tot_match = 0, 0, 0\n",
    "        for arg, count in self.arg_match.items():\n",
    "            excess = self.arg_excess.get(arg, 0)\n",
    "            missed = self.arg_missed.get(arg, 0)\n",
    "            p,r,f = self.get_metrics(false_pos=excess, false_neg=missed, true_pos=count)\n",
    "            processed_args.add(arg)\n",
    "            results.append((arg, count, excess, missed, p, r, f))\n",
    "            tot_excess += excess\n",
    "            tot_missed += missed\n",
    "            tot_match += count\n",
    "        for arg, count in self.arg_excess.items():\n",
    "            if arg not in processed_args:\n",
    "                excess = count\n",
    "                missed = self.arg_missed.get(arg, 0)\n",
    "                correct = self.arg_match.get(arg, 0)\n",
    "                p, r, f = self.get_metrics(false_pos=excess, false_neg=missed, true_pos=correct) # p,r,f = 0,0,0\n",
    "                processed_args.add(arg)\n",
    "                results.append((arg, correct, excess, missed, p, r, f))\n",
    "                tot_excess += excess\n",
    "                tot_missed += missed\n",
    "                tot_match += correct\n",
    "        for arg, count in self.arg_missed.items():\n",
    "            if arg not in processed_args:\n",
    "                excess = self.arg_excess.get(arg, 0)\n",
    "                correct = self.arg_match.get(arg, 0)\n",
    "                missed = count\n",
    "                p, r, f = self.get_metrics(false_pos=excess, false_neg=missed, true_pos=correct) # p,r,f = 0,0,0\n",
    "                results.append((arg, correct, excess, missed, p, r, f))\n",
    "                tot_excess += excess\n",
    "                tot_missed += missed\n",
    "                tot_match += correct\n",
    "        results = sorted(results, key= lambda x: x[0])\n",
    "\n",
    "        prec, rec, F1 = self.get_metrics(false_pos=tot_excess, false_neg=tot_missed, true_pos=tot_match)\n",
    "\n",
    "        if print_metrics:\n",
    "            print(\"\\n--- OVERALL ---\\nCorrect: {0}\\tExcess: {1}\\tMissed: {2}\\nPrecision: {3:.2f}\\t\\tRecall: {4:.2f}\\nF1: {5:.2f}\\n\".format(tot_match, tot_excess, tot_missed, prec, rec, F1))\n",
    "            print(tabulate(results, headers=[\"corr.\", \"excess\", \"missed\", \"prec.\", \"rec.\", \"F1\"], floatfmt=\".2f\"))\n",
    "        if save_to_file:\n",
    "            fout = open(save_to_file, \"w\")\n",
    "            fout.write(\"\\n--- OVERALL ---\\nCorrect: {0}\\tExcess: {1}\\tMissed: {2}\\nPrecision: {3:.2f}\\t\\tRecall: {4:.2f}\\nF1: {5:.2f}\\n\".format(tot_match, tot_excess, tot_missed, prec, rec, F1))\n",
    "            fout.write(tabulate(results, headers=[\"corr.\", \"excess\", \"missed\", \"prec.\", \"rec.\", \"F1\"], floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    def __init__(self, config):\n",
    "        self.name = config[\"model\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.save_location_model = config[\"model_location\"]\n",
    "        self.tokenizer: AutoTokenizer = None\n",
    "        self.model: AutoModelForTokenClassification = None\n",
    "\n",
    "    def build_label_vocab(self) -> dict:\n",
    "        label2index = {\"O\": 0}\n",
    "        for j in range(2):\n",
    "          tag = \"B-\" if j==0 else \"I-\"\n",
    "          for labelset in configurations[\"default\"][\"srl_labels\"]:\n",
    "            if labelset not in label2index:\n",
    "              label2index[tag+labelset] = len(label2index)\n",
    "        return label2index\n",
    "\n",
    "    def create_model(self):\n",
    "        label2index = self.build_label_vocab()\n",
    "        index2label = {v: k for k, v in label2index.items()}\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.name, cache_dir=\"./cache\")\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.name, num_labels=len(label2index), cache_dir=\"./cache\")\n",
    "        self.model.config.finetuning_task = 'token-classification'\n",
    "        self.model.config.id2label = index2label\n",
    "        self.model.config.label2id = label2index\n",
    "        self.model.config.type_vocab_size = 2\n",
    "        # Create a new Embeddings layer, with 2 possible segments IDs instead of 1\n",
    "        self.model.roberta.embeddings.token_type_embeddings = Embedding(2, self.model.config.hidden_size)\n",
    "        self.model.roberta.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=self.model.config.initializer_range)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_model(self, model_dir):\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def expand_label_token(self, original_sentence, original_labels):\n",
    "        tmp_labels = []\n",
    "        txt_sentences = \" \".join(original_sentence)\n",
    "        tokens = self.tokenizer(txt_sentences, padding=\"max_length\", truncation=True, max_length=configurations[\"default\"][\"max_len\"])\n",
    "        sentence = self.tokenizer.tokenize(txt_sentences)\n",
    "        for i, word in enumerate(original_sentence):\n",
    "            word_pieces = self.tokenizer.tokenize(word)\n",
    "            if len(word_pieces) == 1:\n",
    "                tmp_labels.append(original_labels[i])\n",
    "            else:\n",
    "                tmp_labels.append(original_labels[i])\n",
    "                for _ in range (1, len(word_pieces)):\n",
    "                    if original_labels[i] == \"O\":\n",
    "                        tmp_labels.append(\"O\")\n",
    "                    else:\n",
    "                        tmp_labels.append(\"I\" + original_labels[i][1:])\n",
    "        labels = [\"O\"] + tmp_labels + [\"O\"]\n",
    "        length = len(sentence)+2\n",
    "        return tokens, labels, length\n",
    "\n",
    "    def get_data(self, data):\n",
    "        list_input_ids, verb_indicators, all_labels, attention_masks, length_sentences = [], [], [], [], []\n",
    "        for _, obj in enumerate(data):\n",
    "            tokens, labelset, length_sentence = self.expand_label_token(obj[\"words\"], obj[\"arguments\"])\n",
    "            input_ids = tokens[\"input_ids\"]\n",
    "            attent_mask = tokens[\"attention_mask\"]\n",
    "            list_input_ids.append(input_ids)\n",
    "            attention_masks.append(attent_mask)\n",
    "            length_sentences.append(length_sentence)\n",
    "            # Verb Indicator (which predicate to label)\n",
    "            bio_verb = [1 if label[2:] == \"REL\" else 0 for label in labelset]\n",
    "            verb_indicators.append(bio_verb)\n",
    "            all_labels.append(labelset)\n",
    "\n",
    "        return list_input_ids, verb_indicators, all_labels, attention_masks, length_sentences\n",
    "\n",
    "    def load_srl_dataset(self, data):\n",
    "        input_ids, verb_indicators, labels, attention_masks, seq_lengths = self.get_data(data)\n",
    "        label_ixs = []\n",
    "        label2index = self.build_label_vocab()\n",
    "        # Convert label to their indices\n",
    "        for i, labelset in enumerate(labels):\n",
    "            label_ixs.append([label2index.get(l, 1) for l in labelset])\n",
    "        # pad label and verb consequence\n",
    "        input_is_pred = pad_sequences(verb_indicators, maxlen=configurations[\"default\"][\"max_len\"], dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        label_ids = pad_sequences(label_ixs, maxlen=configurations[\"default\"][\"max_len\"], dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "        label_ids = LongTensor(label_ids)\n",
    "        return LongTensor(input_ids), LongTensor(attention_masks), label_ids,  LongTensor(seq_lengths), LongTensor(input_is_pred)\n",
    "\n",
    "    def create_train_val_dataloader(self, train_data, val_data):\n",
    "        train_inputs, train_masks, train_labels, _, train_preds = self.load_srl_dataset(train_data)\n",
    "        # Create the DataLoader for training set.\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels, train_preds)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.batch_size)\n",
    "        val_inputs, val_masks, val_labels, _, val_preds = self.load_srl_dataset(val_data)\n",
    "        # Create the DataLoader for validation set\n",
    "        val_data = TensorDataset(val_inputs, val_masks, val_labels, val_preds)\n",
    "        val_sampler = RandomSampler(val_data)\n",
    "        val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=self.batch_size)\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def create_pred_dataloader(self, data):\n",
    "        prediction_inputs, prediction_masks, gold_labels, seq_lens, gold_predicates = self.load_srl_dataset(data)\n",
    "        # Create the DataLoader.\n",
    "        prediction_data = TensorDataset(prediction_inputs, prediction_masks, gold_labels, seq_lens, gold_predicates)\n",
    "        prediction_sampler = SequentialSampler(prediction_data)\n",
    "        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=self.batch_size)\n",
    "        return prediction_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/indosum_verb_train.csv\")\n",
    "val_data = pd.read_csv(\"data/indosum_verb_validation.csv\")\n",
    "test_data = pd.read_csv(\"data/indosum_verb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for i, text in test_data.iterrows():\n",
    "    new_text = text[\"sentence\"].split(\",\")\n",
    "    new_text = \" ,\".join(new_text)\n",
    "    new_text = new_text.split(\".\")\n",
    "    new_text = \" .\".join(new_text)\n",
    "    new_text = new_text.split(\"?\")\n",
    "    new_text = \" ?\".join(new_text)\n",
    "    sentence.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"sentence\"] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for i, text in test_data.iterrows():\n",
    "    words = text[\"sentence\"].split(\" \")\n",
    "    try:\n",
    "        index = words.index(text[\"verb\"])\n",
    "        correct.append(True)\n",
    "    except ValueError:\n",
    "        correct.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"correct\"] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictModel():\n",
    "    def __init__(self, config):\n",
    "        self.model_dir = config[\"model_location\"]\n",
    "        self.pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "        self.transformer_model: TransformerModel = TransformerModel(config)\n",
    "        self.matrix_model = None\n",
    "\n",
    "    def preprocess_text(self,data):\n",
    "        list_data = []\n",
    "        for i, text in data.iterrows():\n",
    "            words = text[\"sentence\"].split(\" \")\n",
    "            labels = [\"O\" for i in range (len(words))]\n",
    "            index = words.index(text[\"verb\"])\n",
    "            labels[index] = \"B-REL\"\n",
    "            srl = {\n",
    "            \"words\" : words,\n",
    "            \"arguments\" : labels,\n",
    "            \"predicate\" : text[\"verb\"]\n",
    "            }\n",
    "            list_data.append(srl)\n",
    "        return list_data\n",
    "\n",
    "    def generate_label(self, data_text):\n",
    "        data = self.preprocess_text(data_text)\n",
    "        self.matrix_model = MatrixModel()\n",
    "        # Load Saved Model\n",
    "        self.transformer_model.load_model(self.model_dir)\n",
    "        model = self.transformer_model.model\n",
    "        tokenizer =self.transformer_model.tokenizer\n",
    "        label2index = self.transformer_model.build_label_vocab()\n",
    "        index2label = {v: k for k, v in label2index.items()}\n",
    "        list_pred_labels = []\n",
    "        # Load File for Predictions\n",
    "        pred_dataloader = self.transformer_model.create_pred_dataloader(data)\n",
    "        model.eval()\n",
    "        total_sents = 0\n",
    "\n",
    "        for batch in pred_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels, b_lengths, b_preds = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=b_preds, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            class_probabilities = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            # Move class_probabilities and labels to CPU\n",
    "            class_probabilities = class_probabilities.detach().cpu().numpy()\n",
    "            argmax_indices = np.argmax(class_probabilities, axis=-1)\n",
    "\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            seq_lengths = b_lengths.to('cpu').numpy()\n",
    "\n",
    "            for ix in range(len(label_ids)):\n",
    "                total_sents += 1\n",
    "                text = tokenizer.convert_ids_to_tokens(b_input_ids[ix])\n",
    "                # Store predictions and true labels\n",
    "                pred_labels = [index2label[p] for p in argmax_indices[ix][:seq_lengths[ix]]]\n",
    "                gold_labels = [index2label[g] for g in label_ids[ix]]\n",
    "                # Delete unnecessary label\n",
    "                idx_pad = text.index(\"<pad>\")\n",
    "                pred_labels = pred_labels[1:len(pred_labels)-1]\n",
    "                gold_labels = gold_labels[1:idx_pad-1]\n",
    "                text = text[1:idx_pad-1]\n",
    "                _, pred_labels = self.matrix_model.filter_label(text, gold_labels, pred_labels)\n",
    "                list_pred_labels.append(pred_labels)\n",
    "                # print(f\"\\n----- {total_sents} -----\\n{teks}\\n{pred_labels}\")\n",
    "        new_data = data_text\n",
    "        new_data[\"srl\"] = list_pred_labels\n",
    "        new_data.to_csv(\"indosum_srl_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large english model with gozali data 32 batch\n",
    "model_predict = PredictModel(configurations[\"xlmr_32\"])\n",
    "model_predict.generate_label(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"indosum_srl_test.csv\")\n",
    "indosum_data = load_dataset(\"maryantocinn/indosum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split punctuation in every word\n",
    "sentence_summary = []\n",
    "for text in indosum_data[\"test\"]:\n",
    "    new_text = text[\"summary\"].split(\",\")\n",
    "    new_text = \" ,\".join(new_text)\n",
    "    new_text = new_text.split(\".\")\n",
    "    new_text = \" .\".join(new_text)\n",
    "    new_text = new_text.split(\"?\")\n",
    "    new_text = \" ?\".join(new_text)\n",
    "    new_text = new_text.lower()\n",
    "    sentence_summary.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\"\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "model.to(device)\n",
    "\n",
    "def get_word_embeddings(tokens):\n",
    "    inputs = tokenizer(tokens, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.squeeze(0)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "def filter_labels_if_not_in_common(embed1, embed2, labels1, labels2):\n",
    "    common_labels = set(labels1).intersection(set(labels2))\n",
    "\n",
    "    embed1 = [embed for embed, label in zip(embed1, labels1) if label in common_labels]\n",
    "    label1 = [label for label in labels1 if label in common_labels]\n",
    "    embed2 = [embed for embed, label in zip(embed2, labels2) if label in common_labels]\n",
    "    label2 = [label for label in labels2 if label in common_labels]\n",
    "\n",
    "    all_elements = set(labels1).union(set(labels2))\n",
    "\n",
    "    return embed1, embed2, label1, label2, len(all_elements)\n",
    "\n",
    "def filter_words_labels(srl_tag):\n",
    "    filtered_labels = [label for label in srl_tag if label != 'O']\n",
    "    filtered_labels = [label[2:] for label in filtered_labels]\n",
    "    return filtered_labels\n",
    "\n",
    "def filter_words_token(row):\n",
    "    tokens = row[\"sentence\"].split()\n",
    "    labels = row[\"srl\"]\n",
    "    filtered_tokens = [token for token, label in zip(tokens, labels) if label != 'O']\n",
    "    return filtered_tokens\n",
    "\n",
    "def sentence_similarity(embeddings1, embeddings2, labels1, labels2, count):\n",
    "    max_similarities = {}\n",
    "\n",
    "    for i, (emb1, label1) in enumerate(zip(embeddings1, labels1)):\n",
    "        for j, (emb2, label2) in enumerate(zip(embeddings2, labels2)):\n",
    "            if label1 == label2:\n",
    "                # Calculate similarity\n",
    "                similarity = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]\n",
    "\n",
    "                if label1 not in max_similarities or similarity > max_similarities[label1]:\n",
    "                    max_similarities[label1] = similarity\n",
    "\n",
    "    # Sum up the maximum similarities for each label\n",
    "    total_max_similarity = sum(max_similarities.values())\n",
    "\n",
    "    return total_max_similarity / count\n",
    "\n",
    "def count_o_labels(srl_tags):\n",
    "    return srl_tags.count('O')\n",
    "\n",
    "def reduce_same_sentence(data):\n",
    "    data['o_label_count'] = data['srl'].apply(count_o_labels)\n",
    "    df_sorted = data.sort_values(by='o_label_count', ascending=True)\n",
    "    df_reduced = df_sorted.drop_duplicates(subset='sentence', keep='first')\n",
    "    return df_reduced.sort_index()\n",
    "\n",
    "def calculate_sentence_scores(df):\n",
    "    sentence_scores = []\n",
    "    similarity_cache = {}  # Cache to store previously computed similarities\n",
    "    for i, row1 in df.iterrows():\n",
    "        embeddings1, labels1 = row1['embeddings'], row1['srl']\n",
    "        sentence_score = 0\n",
    "        \n",
    "        for j, row2 in df.iterrows():\n",
    "            if i != j:\n",
    "                # Check if this similarity has been computed before\n",
    "                if (i, j) in similarity_cache:\n",
    "                    similarity = similarity_cache[(i, j)]\n",
    "                elif (j, i) in similarity_cache:\n",
    "                    similarity = similarity_cache[(j, i)]\n",
    "                else:\n",
    "                    embeddings2, labels2 = row2['embeddings'], row2['srl']\n",
    "                    embeddings1, embeddings2, labels1, labels2, count = filter_labels_if_not_in_common(embeddings1, embeddings2, labels1, labels2)\n",
    "                    similarity = sentence_similarity(embeddings1, embeddings2, labels1, labels2, count)\n",
    "                    similarity_cache[(i, j)] = similarity\n",
    "                    similarity_cache[(j, i)] = similarity\n",
    "                \n",
    "                sentence_score += similarity\n",
    "        \n",
    "        sentence_scores.append(sentence_score)\n",
    "    \n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 10/10 [00:52<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "length_article = len(data[\"article_id\"].value_counts())\n",
    "list_sentence_hyp = []\n",
    "for i in tqdm(range(10), desc=\"Processing articles\"):\n",
    "    df = data[data[\"article_id\"] == i]\n",
    "    df[\"srl\"] = df[\"srl\"].apply(ast.literal_eval)\n",
    "    df = reduce_same_sentence(df)\n",
    "    df[\"labels\"] =  df['srl'].apply(filter_words_labels)\n",
    "    df[\"token\"] = df.apply(filter_words_token, axis=1)\n",
    "    df['embeddings'] = df[\"token\"].apply(get_word_embeddings)\n",
    "    score = calculate_sentence_scores(df)\n",
    "    df[\"score\"] = score\n",
    "    df.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    top_count = math.ceil(len(df) / 4)\n",
    "    top = df.head(top_count)\n",
    "    top.sort_values(\"sentence_id\", inplace=True)\n",
    "    sentences = \"\"\n",
    "    for i, sentence in top.iterrows():\n",
    "        sentences += \" \" + sentence[\"sentence\"] + \" .\"\n",
    "    list_sentence_hyp.append(sentences.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Scores: ROUGE-1: 0.4062, ROUGE-2: 0.2530, ROUGE-L: 0.3769\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Create a Rouge object\n",
    "rouge = Rouge()\n",
    "f1_scores = []\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "for system, reference in zip(list_sentence_hyp, sentence_summary):\n",
    "    scores = rouge.get_scores(system, reference)[0]  # get_scores returns a list of results\n",
    "    f1_rouge1 = scores['rouge-1']['f']\n",
    "    f1_rouge2 = scores['rouge-2']['f']\n",
    "    f1_rougeL = scores['rouge-l']['f']\n",
    "    \n",
    "    # Collect F1 scores for all three ROUGE metrics\n",
    "    f1_scores.append((f1_rouge1, f1_rouge2, f1_rougeL))\n",
    "\n",
    "# Convert list of tuples to a NumPy array for easy averaging\n",
    "f1_scores_array = np.array(f1_scores)\n",
    "\n",
    "# Calculate average F1 scores for ROUGE-1, ROUGE-2, and ROUGE-L\n",
    "average_f1_scores = np.mean(f1_scores_array, axis=0)\n",
    "print(f\"Average F1 Scores: ROUGE-1: {average_f1_scores[0]:.4f}, ROUGE-2: {average_f1_scores[1]:.4f}, ROUGE-L: {average_f1_scores[2]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
